{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1cee5737-da81-4067-90b5-572ad9573f90",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "from time import time\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3bbc1a5-19dd-43c3-94bb-b9c4fdf98a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.embeddings.sentence_transformer import (\n",
    "    SentenceTransformerEmbeddings,\n",
    ")\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_text_splitters import CharacterTextSplitter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc9c580d-a3f8-44c4-8e61-2a624658b785",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 1004, which is longer than the specified 1000\n",
      "Created a chunk of size 1203, which is longer than the specified 1000\n",
      "Created a chunk of size 1025, which is longer than the specified 1000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# load the document and split it into chunks\n",
    "loader = TextLoader(\"paul_graham/paul_graham_essay.txt\")\n",
    "documents = loader.load()\n",
    "\n",
    "# split it into chunks\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=10)\n",
    "docs = text_splitter.split_documents(documents)\n",
    "\n",
    "# create the open-source embedding function\n",
    "embedding_function = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# load it into Chroma\n",
    "db = Chroma.from_documents(docs, embedding_function)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "efc5f758-ef99-4151-854a-7861ebbda211",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# query it\n",
    "def retrival(query):\n",
    "    \n",
    "    docs = db.similarity_search(query)\n",
    "\n",
    "    # print results\n",
    "    # print(docs)\n",
    "    # print()\n",
    "    # print(docs[0].page_content)\n",
    "    return docs[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e13c6cca-779b-466d-8861-2f1799c082e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.llms import LlamaCpp\n",
    "from langchain_core.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2e75a425-39c3-4238-89a2-f9e5cd7116b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks support token-wise streaming\n",
    "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6adb5e8f-9dd6-4a4f-b9c1-7221528654a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 16 key-value pairs and 291 tensors from /Users/abhaychoudhary/Documents/Abhay/LLAMA2/mode_run/llama.cpp-master/models/7B/ggml-model-q4_0.bin (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = llama-main\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 2048\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 2\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_0:  225 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 2048\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 32\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 4096\n",
      "llm_load_print_meta: n_embd_v_gqa     = 4096\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 11008\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 2048\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_0\n",
      "llm_load_print_meta: model params     = 6.74 B\n",
      "llm_load_print_meta: model size       = 3.56 GiB (4.54 BPW) \n",
      "llm_load_print_meta: general.name     = llama-main\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.22 MiB\n",
      "ggml_backend_metal_buffer_from_ptr: allocated buffer, size =  3577.56 MiB, ( 4795.33 / 12288.02)\n",
      "llm_load_tensors: offloading 32 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 33/33 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =    70.31 MiB\n",
      "llm_load_tensors:      Metal buffer size =  3577.56 MiB\n",
      "..................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 2000\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M3 Pro\n",
      "ggml_metal_init: picking default device: Apple M3 Pro\n",
      "ggml_metal_init: default.metallib not found, loading from source\n",
      "ggml_metal_init: GGML_METAL_PATH_RESOURCES = nil\n",
      "ggml_metal_init: loading '/Users/abhaychoudhary/anaconda3/envs/llama/lib/python3.12/site-packages/llama_cpp/ggml-metal.metal'\n",
      "ggml_metal_init: GPU name:   Apple M3 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 12884.92 MB\n",
      "ggml_backend_metal_buffer_type_alloc_buffer: allocated buffer, size =  1000.00 MiB, ( 5795.58 / 12288.02)\n",
      "llama_kv_cache_init:      Metal KV buffer size =  1000.00 MiB\n",
      "llama_new_context_with_model: KV self size  = 1000.00 MiB, K (f16):  500.00 MiB, V (f16):  500.00 MiB\n",
      "llama_new_context_with_model:        CPU input buffer size   =    12.93 MiB\n",
      "ggml_backend_metal_buffer_type_alloc_buffer: allocated buffer, size =   160.92 MiB, ( 5956.50 / 12288.02)\n",
      "llama_new_context_with_model:      Metal compute buffer size =   160.91 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =     8.00 MiB\n",
      "llama_new_context_with_model: graph splits (measure): 2\n",
      "AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | \n",
      "Model metadata: {'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'llama.attention.head_count_kv': '32', 'llama.context_length': '2048', 'llama.attention.head_count': '32', 'llama.rope.dimension_count': '128', 'general.file_type': '2', 'llama.feed_forward_length': '11008', 'llama.embedding_length': '4096', 'llama.block_count': '32', 'general.architecture': 'llama', 'llama.attention.layer_norm_rms_epsilon': '0.000001', 'general.name': 'llama-main'}\n",
      "Using fallback chat format: None\n"
     ]
    }
   ],
   "source": [
    "n_gpu_layers = -1  # The number of layers to put on the GPU. The rest will be on the CPU. If you don't know how many layers there are, you can use -1 to move all to GPU.\n",
    "n_batch = 512  # Should be between 1 and n_ctx, consider the amount of VRAM in your GPU.\n",
    "\n",
    "# Make sure the model path is correct for your system!\n",
    "model = LlamaCpp(\n",
    "    temperature=0.80,\n",
    "    model_path=\"/Users/abhaychoudhary/Documents/Abhay/LLAMA2/mode_run/llama.cpp-master/models/7B/ggml-model-q4_0.bin\",\n",
    "    n_gpu_layers=n_gpu_layers,\n",
    "    n_batch=n_batch,\n",
    "    max_tokens=2000,\n",
    "    callback_manager=callback_manager,\n",
    "    verbose=True, \n",
    "    n_ctx=2000,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "336472ea-6e18-4fc5-9c31-b37e503ac173",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.chains.question_answering import load_qa_chain\n",
    "chain=load_qa_chain(model, chain_type=\"stuff\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ac8284ae-1e83-4c59-be0a-5256e1ff9d9c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Then some online stores started to appear, and I realized that except for the order buttons they were identical to the sites we\\'d been generating for galleries. This impressive-sounding thing called an \"internet storefront\" was something we already knew how to build.\\n\\nSo in the summer of 1995, after I submitted the camera-ready copy of ANSI Common Lisp to the publishers, we started trying to write software to build online stores. At first this was going to be normal desktop software, which in those days meant Windows software. That was an alarming prospect, because neither of us knew how to write Windows software or wanted to learn. We lived in the Unix world. But we decided we\\'d at least try writing a prototype store builder on Unix. Robert wrote a shopping cart, and I wrote a new site generator for stores — in Lisp, of course.', metadata={'source': 'paul_graham/paul_graham_essay.txt'}),\n",
       " Document(page_content=\"We originally hoped to launch in September, but we got more ambitious about the software as we worked on it. Eventually we managed to build a WYSIWYG site builder, in the sense that as you were creating pages, they looked exactly like the static ones that would be generated later, except that instead of leading to static pages, the links all referred to closures stored in a hash table on the server.\\n\\nIt helped to have studied art, because the main goal of an online store builder is to make users look legit, and the key to looking legit is high production values. If you get page layouts and fonts and colors right, you can make a guy running a store out of his bedroom look more legit than a big company.\\n\\n(If you're curious why my site looks so old-fashioned, it's because it's still made with this software. It may look clunky today, but in 1996 it was the last word in slick.)\", metadata={'source': 'paul_graham/paul_graham_essay.txt'}),\n",
       " Document(page_content=\"We opened for business, with 6 stores, in January 1996. It was just as well we waited a few months, because although we worried we were late, we were actually almost fatally early. There was a lot of talk in the press then about ecommerce, but not many people actually wanted online stores. [8]\\n\\nThere were three main parts to the software: the editor, which people used to build sites and which I wrote, the shopping cart, which Robert wrote, and the manager, which kept track of orders and statistics, and which Trevor wrote. In its time, the editor was one of the best general-purpose site builders. I kept the code tight and didn't have to integrate with any other software except Robert's and Trevor's, so it was quite fun to work on. If all I'd had to do was work on this software, the next 3 years would have been the easiest of my life. Unfortunately I had to do a lot more, all of it stuff I was worse at than programming, and the next 3 years were instead the most stressful.\", metadata={'source': 'paul_graham/paul_graham_essay.txt'}),\n",
       " Document(page_content=\"This kind of software, known as a web app, is common now, but at the time it wasn't clear that it was even possible. To find out, we decided to try making a version of our store builder that you could control through the browser. A couple days later, on August 12, we had one that worked. The UI was horrible, but it proved you could build a whole store through the browser, without any client software or typing anything into the command line on the server.\\n\\nNow we felt like we were really onto something. I had visions of a whole new generation of software working this way. You wouldn't need versions, or ports, or any of that crap. At Interleaf there had been a whole group called Release Engineering that seemed to be at least as big as the group that actually wrote the software. Now you could just update the software right on the server.\", metadata={'source': 'paul_graham/paul_graham_essay.txt'})]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "query=\"who wrote software to generate web sites\"\n",
    "docs=db.similarity_search(query)\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8f3b085f-0d85-4e9e-b0c2-a0e8f3d82d60",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/abhaychoudhary/anaconda3/envs/llama/lib/python3.12/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `run` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Robert wrote a shopping cart, and I wrote a new site generator for stores — in Lisp, of course."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    4081.34 ms\n",
      "llama_print_timings:      sample time =       2.95 ms /    25 runs   (    0.12 ms per token,  8465.97 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5607.20 ms /   898 tokens (    6.24 ms per token,   160.15 tokens per second)\n",
      "llama_print_timings:        eval time =     882.17 ms /    24 runs   (   36.76 ms per token,    27.21 tokens per second)\n",
      "llama_print_timings:       total time =    6547.04 ms /   922 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' Robert wrote a shopping cart, and I wrote a new site generator for stores — in Lisp, of course.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.run(input_documents=docs, question=query,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "099f2f17-c0a5-4234-8bc5-0104fcb89142",
   "metadata": {},
   "source": [
    "### Chain for prompt engineering for RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cd9df34e-845b-47fb-832a-bc5a8adb58ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "<s>[INST] <<SYS>>\n",
    "You are a helpful AI assistant.\n",
    "Answer based on the context provided. If you cannot find the correct answer, say I don't know. Be concise max 100 words and just include the response if it is correct\n",
    "<</SYS>>\n",
    "{context}\n",
    "Question: {question}\n",
    "Helpful Answer: [/INST]\n",
    "\"\"\"\n",
    "prompt = PromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a9706d11-529b-428e-a827-8b0b4006ae63",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever=db.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "978bb9e4-f91b-45f6-b54a-afcd861b5d4b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()} \n",
    "    | prompt \n",
    "    | model \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "79a567c3-bde7-4359-b217-c06cb49fb9e3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Robert wrote a shopping cart, Paul wrote an editor, and Trevor wrote a manager."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    4081.34 ms\n",
      "llama_print_timings:      sample time =       2.04 ms /    21 runs   (    0.10 ms per token, 10319.41 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3853.07 ms /  1047 tokens (    3.68 ms per token,   271.73 tokens per second)\n",
      "llama_print_timings:        eval time =     751.62 ms /    20 runs   (   37.58 ms per token,    26.61 tokens per second)\n",
      "llama_print_timings:       total time =    4650.58 ms /  1067 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Robert wrote a shopping cart, Paul wrote an editor, and Trevor wrote a manager.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\"who wrote software to generate web sites\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a1e10dcd-67ae-422e-bc5e-dfb7a61f5535",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ", I'm a big fan of your work.ἱ\n",
      "I have been following you for years and I must say that your talent is out of this world. Your ability to create such complex and intricate designs with ease is truly remarkable. I can only imagine the countless hours you must put in to produce such masterpieces.\n",
      "I am particularly impressed by your latest project, the new website for XYZ Corporation. The layout is clean and modern, and the graphics are simply stunning. I love how you were able to incorporate the company's branding throughout the site without making it feel too repetitive or obvious. It really feels like a cohesive and well-thought-out design.\n",
      "As someone who has worked in the industry for many years, I can appreciate just how difficult it is to create a website that not only looks great but also functions seamlessly on all devices. You have clearly put in the time and effort to ensure that every aspect of the site works flawlessly, from the responsive layout to the easy-to-use navigation menu. It's clear that you are dedicated to your craft and committed to delivering nothing but the best to your clients.\n",
      "I must admit that I am a bit jealous of your skills. I wish I had half the talent and creativity that you possess! But I am always eager to learn from the best, so I will be keeping an eye on your future projects. Who knows, maybe one day I will be able to create something half as beautiful as what you have produced.\n",
      "In any case, I just wanted to take a moment to express my admiration for your work. You truly are a master of your craft, and I am sure that your clients are lucky to have you on their side. Keep up the fantastic work!\n",
      "Sincerely,\n",
      "Jane Doe"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    4081.34 ms\n",
      "llama_print_timings:      sample time =      37.73 ms /   389 runs   (    0.10 ms per token, 10311.19 tokens per second)\n",
      "llama_print_timings: prompt eval time =     238.63 ms /     7 tokens (   34.09 ms per token,    29.33 tokens per second)\n",
      "llama_print_timings:        eval time =   13050.37 ms /   388 runs   (   33.63 ms per token,    29.73 tokens per second)\n",
      "llama_print_timings:       total time =   14088.38 ms /   395 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\", I'm a big fan of your work.ἱ\\nI have been following you for years and I must say that your talent is out of this world. Your ability to create such complex and intricate designs with ease is truly remarkable. I can only imagine the countless hours you must put in to produce such masterpieces.\\nI am particularly impressed by your latest project, the new website for XYZ Corporation. The layout is clean and modern, and the graphics are simply stunning. I love how you were able to incorporate the company's branding throughout the site without making it feel too repetitive or obvious. It really feels like a cohesive and well-thought-out design.\\nAs someone who has worked in the industry for many years, I can appreciate just how difficult it is to create a website that not only looks great but also functions seamlessly on all devices. You have clearly put in the time and effort to ensure that every aspect of the site works flawlessly, from the responsive layout to the easy-to-use navigation menu. It's clear that you are dedicated to your craft and committed to delivering nothing but the best to your clients.\\nI must admit that I am a bit jealous of your skills. I wish I had half the talent and creativity that you possess! But I am always eager to learn from the best, so I will be keeping an eye on your future projects. Who knows, maybe one day I will be able to create something half as beautiful as what you have produced.\\nIn any case, I just wanted to take a moment to express my admiration for your work. You truly are a master of your craft, and I am sure that your clients are lucky to have you on their side. Keep up the fantastic work!\\nSincerely,\\nJane Doe\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.invoke(\"who wrote software to generate web sites\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd631d3a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama",
   "language": "python",
   "name": "llama"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
